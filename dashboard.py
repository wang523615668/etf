# dashboard.py (V21.1 - ä¿®æ­£è¯­æ³•é”™è¯¯ï¼Œå®Œå–„äº¤æ˜“ç®¡ç†)

import streamlit as st
import pandas as pd
import numpy as np 
import json
import os
from datetime import datetime, timedelta
import time
import glob 

# ================= é…ç½®åŒºåŸŸ (V21.1) =================

# å®Œæ•´çš„æŒ‡æ•°åˆ—è¡¨
TARGETS = {
    "å¤§ç›˜.csv": "å¤§ç›˜æŒ‡æ•°",
    "æ²ªæ·±300.csv": "æ²ªæ·±300æŒ‡æ•°",
    "ä¸­è¯500.csv": "ä¸­è¯500æŒ‡æ•°",
    
    # å…¶ä»–æŒ‡æ•°
    "å…¨æŒ‡åŒ»è¯.csv": "å…¨æŒ‡åŒ»è¯",
    "ä¸Šè¯50.csv": "ä¸Šè¯50",
    "åˆ›ä¸šæ¿æŒ‡.csv": "åˆ›ä¸šæ¿æŒ‡", 
    "å…»è€äº§ä¸š.csv": "å…»è€äº§ä¸š",
    "ä¸­è¯çº¢åˆ©.csv": "ä¸­è¯çº¢åˆ©",
    "ä¸­è¯ç¯ä¿.csv": "ä¸­è¯ç¯ä¿",
    "ä¸­è¯ä¼ åª’.csv": "ä¸­è¯ä¼ åª’",
    "å…¨æŒ‡é‡‘è.csv": "å…¨æŒ‡é‡‘è",
    "è¯åˆ¸å…¬å¸.csv": "è¯åˆ¸å…¬å¸",
    "å…¨æŒ‡æ¶ˆè´¹.csv": "å…¨æŒ‡æ¶ˆè´¹",
    "å…¨æŒ‡ä¿¡æ¯.csv": "å…¨æŒ‡ä¿¡æ¯",
    "ä¸­è¯åŒ»ç–—.csv": "ä¸­è¯åŒ»ç–—",
    "ä¸­è¯ç™½é…’.csv": "ä¸­è¯ç™½é…’",
}

# é™åˆ¶è¯¦æƒ…å’Œå›¾è¡¨æ˜¾ç¤ºçš„æœ€åˆä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ•° (åœ¨ V21.1 ä¸­ä¸»è¦ç”¨äºä¸»é¡µæ•°æ®å±•ç¤º)
INITIAL_TARGETS = {
    "å¤§ç›˜.csv": "å¤§ç›˜æŒ‡æ•°",
    "æ²ªæ·±300.csv": "æ²ªæ·±300æŒ‡æ•°",
    "ä¸­è¯500.csv": "ä¸­è¯500æŒ‡æ•°",
}

DATA_DIR = "index_data"
STATE_FILE = "portfolio_status.json" 

MAX_UNITS = 10          
STEP_PERCENT = 0.06     
MIN_INTERVAL_DAYS = 30 
VOLATILITY_OVERRIDE_PCT = 0.12 

# ================= æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================

def load_state():
    """åŠ è½½æœ¬åœ°æŒä»“çŠ¶æ€"""
    if os.path.exists(STATE_FILE):
        try:
            state = json.load(open(STATE_FILE, 'r', encoding='utf-8'))
            for code in TARGETS.keys():
                if code not in state:
                     state[code] = {"holdings": 0, "history": []}
                elif "history" not in state[code]:
                     state[code]["history"] = []
            return state
        except json.JSONDecodeError:
            print("è­¦å‘Š: çŠ¶æ€æ–‡ä»¶æŸåï¼Œå·²é‡ç½®ã€‚")
            return {code: {"holdings": 0, "history": []} for code in TARGETS.keys()}
    return {code: {"holdings": 0, "history": []} for code in TARGETS.keys()}

def save_state(state):
    """ä¿å­˜æœ¬åœ°æŒä»“çŠ¶æ€"""
    # æ¯æ¬¡ä¿å­˜å‰ï¼Œé‡æ–°è®¡ç®— holdingsï¼Œç¡®ä¿å…¶ä¸ history ä¿æŒä¸€è‡´
    recalculate_holdings(state)
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=4)

def recalculate_holdings(state):
    """æ ¹æ®äº¤æ˜“å†å²é‡æ–°è®¡ç®—å½“å‰æŒä»“ä»½æ•°"""
    for code, data in state.items():
        if 'history' in data:
            total_units = 0
            for transaction in data['history']:
                unit = transaction.get('unit', 1) 
                if transaction.get('type') == 'ä¹°å…¥':
                    total_units += unit
                elif transaction.get('type') == 'å–å‡º':
                    total_units -= unit
            # ç¡®ä¿æŒä»“ä»½æ•°ä¸ä½äºé›¶
            state[code]['holdings'] = max(0, total_units)
    return state

def find_pe_by_date(df, target_date_str):
    """æ ¹æ®æ—¥æœŸæŸ¥æ‰¾å¯¹åº”çš„ PE å€¼å’Œæ”¶ç›˜ç‚¹ä½ã€‚"""
    try:
        target_date = pd.to_datetime(target_date_str)
        date_series = pd.to_datetime(df['Date'], errors='coerce') 
        row = df[date_series.dt.normalize() == target_date.normalize()]
        if not row.empty:
            close_price_col = 'æ”¶ç›˜' if 'æ”¶ç›˜' in df.columns else 'Close' 
            return row.iloc[0]['pe'], row.iloc[0][close_price_col]
        return np.nan, np.nan
    except Exception as e:
        return np.nan, np.nan

def find_latest_data_file(prefix):
    """æŸ¥æ‰¾åŒ¹é…å‰ç¼€çš„æœ€æ–°æ•°æ®æ–‡ä»¶ï¼Œå¹¶è¿”å›æ–‡ä»¶è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ (æ”¯æŒæ¨¡ç³ŠåŒ¹é…)"""
    search_pattern = os.path.join(DATA_DIR, f"{prefix}_*.csv")
    matching_files = glob.glob(search_pattern)
    
    actual_file_path = None
    file_source_name = None
    last_modified_time = None
    
    if matching_files:
        actual_file_path = max(matching_files, key=os.path.getmtime)
        file_source_name = os.path.basename(actual_file_path)
        last_modified_time = datetime.fromtimestamp(os.path.getmtime(actual_file_path)).strftime('%Y-%m-%d %H:%M:%S')
    else:
        # æŸ¥æ‰¾å›ºå®šæ–‡ä»¶åä½œä¸ºå¤‡ä»½
        fixed_path = os.path.join(DATA_DIR, f"{prefix}.csv")
        if os.path.exists(fixed_path):
            actual_file_path = fixed_path
            file_source_name = f"{prefix}.csv"
            last_modified_time = datetime.fromtimestamp(os.path.getmtime(fixed_path)).strftime('%Y-%m-%d %H:%M:%S')
        
    return actual_file_path, file_source_name, last_modified_time


@st.cache_data(ttl=3600)
def get_metrics_from_csv(file_path):
    """ä»æœ¬åœ° CSV æ–‡ä»¶è¯»å– PE æ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡ã€‚"""
    if not os.path.exists(file_path): return None
    try:
        df = pd.read_csv(file_path, encoding='utf-8', sep=',')
        if len(df) == 0: return None
        df = df.rename(columns={'PE-TTMæ­£æ•°ç­‰æƒ': 'pe', 'æ—¥æœŸ': 'Date', 'PE-TTM åˆ†ä½ç‚¹': 'pe_percentile', 'æ”¶ç›˜ç‚¹ä½': 'Close', 'æ”¶ç›˜': 'Close'})
        for col in ['pe', 'pe_percentile', 'Close']:
             if col in df.columns:
                df[col] = df[col].astype(str).str.replace(r'[^\d.]', '', regex=True)
                df[col] = pd.to_numeric(df[col], errors='coerce') 
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        df = df[['Date', 'pe', 'pe_percentile', 'Close']].dropna(subset=['pe', 'Date', 'pe_percentile', 'Close'])
        df = df.sort_values('Date', ascending=True).reset_index(drop=True)
        if df.empty: return None
        df = df.set_index('Date')
        WINDOW_3Y = '1095D'; WINDOW_5Y = '1825D'; 
        df['avg_3yr_roll'] = df['pe'].rolling(window=WINDOW_3Y, min_periods=1, closed='left').mean()
        df['avg_5yr_roll'] = df['pe'].rolling(window=WINDOW_5Y, min_periods=1, closed='left').mean()
        df['benchmark_roll'] = df['avg_3yr_roll'] 
        df['deviation_pct'] = (df['pe'] - df['benchmark_roll']) / df['benchmark_roll'] * 100
        
        max_dev = df['deviation_pct'].max(); min_dev = df['deviation_pct'].min()
        
        if not np.isnan(max_dev): max_dev_date = df[df['deviation_pct'] == max_dev].iloc[-1].name.strftime('%Y-%m-%d')
        else: max_dev_date = 'N/A'
        if not np.isnan(min_dev): min_dev_date = df[df['deviation_pct'] == min_dev].iloc[-1].name.strftime('%Y-%m-%d')
        else: min_dev_date = 'N/A'
        
        avg_3yr = df['avg_3yr_roll'].iloc[-1]; avg_5yr = df['avg_5yr_roll'].iloc[-1]; avg_10yr = np.nan 
        df = df.reset_index().rename(columns={'index': 'Date'}); df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
        current_pe = df.iloc[-1]['pe']; current_percentile = df.iloc[-1]['pe_percentile']
        
        return (current_pe, current_percentile, avg_3yr, avg_5yr, avg_10yr, 
                df, max_dev, min_dev, max_dev_date, min_dev_date)
        
    except Exception as e:
        st.error(f"[{file_path}] è¯»å–æˆ–å¤„ç†æ•°æ®å¤±è´¥: {e}")
        return None

def calculate_portfolio_metrics(s, current_close_index):
    """è®¡ç®—å¹³å‡æˆæœ¬å’Œæµ®åŠ¨ç›ˆäº (ä½¿ç”¨ ETF å®é™…ä»·æ ¼ä¼°ç®—)"""
    buy_records = [t for t in s['history'] if t['type'] == 'ä¹°å…¥' and 'price' in t and t['price'] is not None]
    
    if not buy_records:
        return np.nan, np.nan 

    total_cost = sum(t['price'] * t['unit'] for t in buy_records)
    total_units_bought = sum(t['unit'] for t in buy_records)
    
    avg_cost = total_cost / total_units_bought if total_units_bought > 0 else np.nan
    
    # å¦‚æœæ²¡æœ‰ä¹°å…¥æˆ–å½“å‰æŒä»“ä¸º0ï¼Œç›ˆäºæ— æ³•è®¡ç®—
    if s['holdings'] == 0:
        return avg_cost, np.nan
    if np.isnan(avg_cost):
        return np.nan, np.nan
    
    # æ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡ä¹°å…¥çš„æŒ‡æ•°ç‚¹ä½ (ç”¨äºä¼°ç®—å½“å‰ ETF ä»·æ ¼)
    last_buy_index_close = next((t['close'] for t in reversed(s['history']) if t.get('type') == 'ä¹°å…¥' and t.get('close') is not None), np.nan)
    
    if not np.isnan(last_buy_index_close) and last_buy_index_close > 0:
        # æ ¸å¿ƒå‡è®¾ï¼šETFä»·æ ¼æ³¢åŠ¨ä¸æŒ‡æ•°ç‚¹ä½æ³¢åŠ¨ä¸€è‡´ï¼Œç”¨æŒ‡æ•°ç‚¹ä½æ¯”ä¾‹ä¼°ç®—å½“å‰ ETF ä»·æ ¼
        estimated_current_etf_price = avg_cost * (current_close_index / last_buy_index_close)
        floating_pl_pct = (estimated_current_etf_price / avg_cost) - 1
    else:
        floating_pl_pct = np.nan 
        
    return avg_cost, floating_pl_pct

def get_full_index_metrics(index_key, state, full_data_frames):
    """
    è·å–å•ä¸ªæŒ‡æ•°çš„å®Œæ•´æŒ‡æ ‡ã€æŒä»“å’Œç›ˆäºä¿¡æ¯ï¼Œç”¨äºå­é¡µé¢è°ƒç”¨ã€‚
    
    è¿”å›: dict (åŒ…å« current_pe, current_close, holdings, avg_cost, pl_pct, df_full, history)
    """
    
    result = {
        "current_pe": np.nan, "current_close": np.nan, "holdings": 0, 
        "avg_cost": np.nan, "pl_pct": np.nan, "df_full": None, 
        "history": state.get(index_key, {}).get("history", [])
    }
    
    df_full = full_data_frames.get(index_key)
    # ç¡®ä¿æŒä»“æ˜¯æœ€æ–°çš„
    s = state.get(index_key, {})
    result["holdings"] = s.get("holdings", 0)
    
    # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰æ•°æ®ï¼Œå°è¯•åŠ è½½
    if df_full is None:
        prefix = index_key.split('.')[0]
        actual_file_path, _, _ = find_latest_data_file(prefix)
        metrics_result = get_metrics_from_csv(actual_file_path)
        if metrics_result:
            df_full = metrics_result[5]
            result["current_pe"] = metrics_result[0]
            result["current_close"] = df_full.iloc[-1]['Close']
            
    if df_full is not None and not df_full.empty:
        result["df_full"] = df_full
        result["current_pe"] = df_full.iloc[-1]['pe']
        result["current_close"] = df_full.iloc[-1]['Close']
        
        avg_cost, pl_pct = calculate_portfolio_metrics(s, result["current_close"])
        result["avg_cost"] = avg_cost
        result["pl_pct"] = pl_pct
        
    return result

# ================= é¢œè‰²é«˜äº®å‡½æ•° =================

def highlight_percentile(val):
    """æ ¹æ®ä¼°å€¼ç™¾åˆ†ä½è¿”å›èƒŒæ™¯é¢œè‰²æ ·å¼"""
    try:
        if isinstance(val, str) and val.endswith('%'):
            pct = float(val.strip('%'))
        else: return '' 
            
        if pct < 20: return 'background-color: #d4edda; color: #155724; font-weight: bold' 
        elif 20 <= pct <= 50: return 'background-color: #fff3cd; color: #856404;' 
        elif 50 < pct <= 80: return 'background-color: #f8d7da; color: #721c24;' 
        elif pct > 80: return 'background-color: #dc3545; color: white; font-weight: bold' 
        else: return ''
    except: return ''

def highlight_signal(val):
    """æ ¹æ®å»ºè®®ä¿¡å·è¿”å›èƒŒæ™¯é¢œè‰²æ ·å¼"""
    if 'ä¹°å…¥' in str(val):
        return f'background-color: #d4edda; color: #155724; font-weight: bold' 
    elif 'å–å‡º' in str(val):
        return f'background-color: #f8d7da; color: #721c24; font-weight: bold'
    elif 'æ•°æ®ç§¯ç´¯ä¸­' in str(val):
         return f'background-color: #fffac0; color: #b58d09; font-weight: bold'
    elif 'è·Œå¹…ä¸è¶³ 6%' in str(val):
         return 'background-color: #e0f7fa; color: #00796b; font-weight: bold' 
    elif '30å¤©/Â±12%é™åˆ¶' in str(val): 
         return 'background-color: #e9ecef; color: #495057; font-weight: bold'
    else:
        return 'background-color: #f0f0f0; color: black;'

def highlight_pl(val):
    """æ ¹æ®æµ®åŠ¨ç›ˆäºè¿”å›èƒŒæ™¯é¢œè‰²æ ·å¼"""
    try:
        if isinstance(val, str) and val.endswith('%'):
            pct = float(val.strip('%'))
        else: return '' 
            
        if pct > 0: return 'color: #155724; font-weight: bold' # ç»¿è‰²å­—ä½“
        elif pct < 0: return 'color: #721c24; font-weight: bold' # çº¢è‰²å­—ä½“
        else: return ''
    except: return ''

# ================= é¡µé¢å¸ƒå±€ï¼ˆä¸»ä½“é€»è¾‘ï¼‰ =================

st.set_page_config(page_title="æŒ‡æ•°å®šæŠ•çœ‹æ¿", layout="wide", page_icon="ğŸ“ˆ")

# --- é¡µé¢å¤´éƒ¨ç¾åŒ– (æ·»åŠ æ•°æ®æ›´æ–°æ—¶é—´) ---
with st.container(border=True):
    st.markdown("## ğŸ“Š æ™ºèƒ½å®šæŠ•çœ‹æ¿ï¼šä¼°å€¼æ€»è§ˆä¸ç­–ç•¥å»ºè®® (V21.1 - äº¤æ˜“ç®¡ç†)")
    st.markdown(f"**ç­–ç•¥é™åˆ¶**: æ¯æ¬¡æ“ä½œé—´éš”éœ€å¤§äº {MIN_INTERVAL_DAYS} å¤©ï¼Œé™¤é PE æ³¢åŠ¨å¹…åº¦å¤§äº {VOLATILITY_OVERRIDE_PCT*100:.0f}%ã€‚")

# --- ä¾§è¾¹æ å’Œæ•°æ®æ–°é²œåº¦æ£€æŸ¥ ---
st.sidebar.header("ğŸ•¹ï¸ æ“ä½œé¢æ¿")
st.sidebar.info(f"æ•°æ®ç›®å½•: **{DATA_DIR}/**")

# æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶ï¼Œè·å–æ›´æ–°æ—¶é—´
latest_modified_time = None
for prefix in [f.split('.')[0] for f in TARGETS.keys()]:
    _, _, mod_time = find_latest_data_file(prefix)
    if mod_time:
        if latest_modified_time is None or mod_time > latest_modified_time:
            latest_modified_time = mod_time

if latest_modified_time:
    st.sidebar.markdown(f"**æœ€åæ•°æ®æ›´æ–°æ—¶é—´ï¼š** `{latest_modified_time}`")
else:
    st.sidebar.warning("âš ï¸ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ index_data æ–‡ä»¶å¤¹ã€‚")

# åŠ è½½çŠ¶æ€ (é‡è¦: è¿è¡Œæ—¶ä¼šé‡æ–°è®¡ç®— holdings)
state = load_state()
state = recalculate_holdings(state)
# é¦–æ¬¡è¿è¡Œæˆ– state å˜æ›´åä¿å­˜ï¼Œç¡®ä¿ holdings æ­£ç¡®
# save_state(state) # é¿å…åœ¨ rerurn ä¹‹å‰é‡å¤ä¿å­˜

# ä¸»æ•°æ®è¡¨æ ¼æ„å»º
table_data = []
decision_logs = {} 

progress_bar = st.progress(0, text="è®¡ç®—ä¸­...")
total_targets = len(TARGETS)
full_data_frames = {} 

# --- å¤§ç›˜æ•´ä½“ä»“ä½æŒ‡æ ‡è®¡ç®— ---
overall_position_pct = 0
overall_pe_percentile = np.nan

if "å¤§ç›˜.csv" in TARGETS:
    prefix = "å¤§ç›˜"
    actual_file_path, file_source_name, _ = find_latest_data_file(prefix)
    if actual_file_path:
        metrics_result_overall = get_metrics_from_csv(actual_file_path)
        if metrics_result_overall:
            _, overall_pe_percentile, _, _, _, _, _, _, _, _ = metrics_result_overall
            if not np.isnan(overall_pe_percentile):
                overall_position_pct = (1 - overall_pe_percentile) * 100
    
# --- æ•°æ®åŠ è½½ã€è‡ªåŠ¨å›å¡«å’ŒæŒ‡æ ‡è®¡ç®— ---
updated_records_count = 0 

for i, (fixed_filename_key, name) in enumerate(TARGETS.items()): 
    
    prefix = fixed_filename_key.split('.')[0]
    progress_bar.progress((i + 1) / total_targets, text=f"æ­£åœ¨å¤„ç† {name} ({i+1}/{total_targets}) - åŒ¹é…æ–‡ä»¶...")
    
    actual_file_path, file_source_name, _ = find_latest_data_file(prefix)
        
    if not actual_file_path:
        continue

    metrics_result = get_metrics_from_csv(actual_file_path) 
    code = fixed_filename_key
    s = state[code]
    current_holdings = s["holdings"]
    days_since_last_op_display = 'â€”'
    avg_cost_display = 'â€”'
    pl_pct_display = 'â€”'

    current_decision_log = [] 

    if metrics_result:
        (curr_pe, curr_percentile, avg3, avg_5yr, avg_10yr, df_full, 
         max_dev, min_dev, max_dev_date, min_dev_date) = metrics_result
        
        full_data_frames[fixed_filename_key] = df_full 

        # --- è‡ªåŠ¨å›å¡«ç¼ºå¤±æ•°æ® ---
        for trade in s['history']:
            if trade['pe'] is None or trade['close'] is None:
                new_pe, new_close = find_pe_by_date(df_full, trade['date'])
                if not np.isnan(new_pe) and not np.isnan(new_close):
                    trade['pe'] = round(new_pe, 2)
                    trade['close'] = round(new_close, 2)
                    updated_records_count += 1
        
        # --- P&L è®¡ç®— ---
        current_close_index = df_full.iloc[-1]['Close']
        avg_cost, pl_pct = calculate_portfolio_metrics(s, current_close_index)

        if not np.isnan(avg_cost):
            avg_cost_display = f"{avg_cost:.4f}"
        if not np.isnan(pl_pct):
            pl_pct_display = f"{pl_pct * 100:.2f}%"

        # --- é˜¶æ¢¯ä¹°å…¥/æ—¶é—´é™åˆ¶åˆ¤æ–­ ---
        last_op = s["history"][-1] if s["history"] else None
        time_limit_suppression = False
        
        if last_op and 'pe' in last_op and last_op['pe'] is not None and not np.isnan(last_op['pe']):
            last_op_date_str = last_op.get("date", datetime.now().strftime("%Y-%m-%d"))
            last_op_date = datetime.strptime(last_op_date_str, "%Y-%m-%d").date()
            days_since_last_op = (datetime.now().date() - last_op_date).days
            days_since_last_op_display = str(days_since_last_op)
            
            current_decision_log.append(f"ä¸Šæ¬¡æ“ä½œè·ä»Š: {days_since_last_op} å¤© (è¦æ±‚ â‰¥ {MIN_INTERVAL_DAYS} å¤©)")
            
            if days_since_last_op < MIN_INTERVAL_DAYS:
                last_op_pe = last_op['pe']
                pe_change_pct = (curr_pe - last_op_pe) / last_op_pe
                current_decision_log.append(f"ä¸Šæ¬¡æ“ä½œPE: {last_op_pe:.2f}, å½“å‰PE: {curr_pe:.2f}, å˜åŠ¨: {pe_change_pct*100:.1f}% (è¦æ±‚ Â±{VOLATILITY_OVERRIDE_PCT*100:.0f}% è¦†ç›–)")
                if abs(pe_change_pct) < VOLATILITY_OVERRIDE_PCT:
                    time_limit_suppression = True
                    current_decision_log.append("ç»“æœ: æ—¶é—´/æ³¢åŠ¨ç‡é™åˆ¶ç”Ÿæ•ˆï¼ŒæŠ‘åˆ¶æ“ä½œã€‚")
                else:
                    current_decision_log.append("ç»“æœ: æ³¢åŠ¨ç‡è¾¾åˆ°è¦†ç›–æ¡ä»¶ï¼Œç»§ç»­è¯„ä¼°ã€‚")
            else:
                current_decision_log.append("ç»“æœ: å·²è¶…è¿‡æœ€å°æ—¶é—´é—´éš”ï¼Œç»§ç»­è¯„ä¼°ã€‚")
        else:
             current_decision_log.append("æ— ä¸Šæ¬¡æ“ä½œè®°å½•ï¼Œä¸æ£€æŸ¥æ—¶é—´/æ³¢åŠ¨ç‡é™åˆ¶ã€‚")
        
        # --- æŸ¥æ‰¾ä¸Šæ¬¡ä¹°å…¥çš„PE (ç”¨äº6%é˜¶æ¢¯ä¹°å…¥æ£€æŸ¥) ---
        last_buy_pe = None
        for trade in reversed(s["history"]):
            if trade['type'] == 'ä¹°å…¥' and trade['pe'] is not None and not np.isnan(trade['pe']):
                last_buy_pe = trade['pe']
                break
        
        last_op_hist = s["history"][-1] if s["history"] else {"date": "1900-01-01", "pe": 0, "close": 0}
        last_date = last_op_hist.get("date", "1900-01-01")
        last_pe = last_op_hist.get("pe") 
        if last_pe is None: last_pe = np.nan

        benchmark_pe = avg3 if not np.isnan(avg3) else 0 
        diff_pct = (curr_pe - benchmark_pe) / benchmark_pe * 100 if benchmark_pe > 0 else np.nan
        signal = "è§‚æœ›"
        
        # ==================== ä¿¡å·åˆ¤æ–­é€»è¾‘ (å«å†³ç­–æ—¥å¿—) ====================
        current_decision_log.append("--- ç­–ç•¥è¯„ä¼° ---")

        if benchmark_pe == 0 or np.isnan(benchmark_pe):
            signal = "âš ï¸ æ•°æ®ç§¯ç´¯ä¸­ (ä¸æ»¡ 3 å¹´)"
            current_decision_log.append(f"æ¡ä»¶: 3å¹´å‡å€¼ PE ({benchmark_pe:.2f}) ä¸è¶³ï¼Œæ— æ³•è¯„ä¼°ã€‚")
        
        elif time_limit_suppression:
            signal = f"â¸ï¸ è§‚æœ› ({MIN_INTERVAL_DAYS}å¤©/Â±{VOLATILITY_OVERRIDE_PCT*100:.0f}%é™åˆ¶)" 
            current_decision_log.append("æ¡ä»¶: è¢«æ—¶é—´/æ³¢åŠ¨ç‡é™åˆ¶æŠ‘åˆ¶ã€‚")

        else:
            current_decision_log.append(f"å½“å‰PE: {curr_pe:.2f}, PEåˆ†ä½ç‚¹: {curr_percentile*100:.1f}%, 3å¹´å‡å€¼PE: {avg3:.2f}")

            buy_condition_1 = curr_percentile < 0.20 
            buy_condition_2 = (not np.isnan(avg3) and curr_pe < avg3) and (not np.isnan(avg_5yr) and curr_pe < avg_5yr) 
            
            sell_condition_1 = curr_percentile > 0.75 
            sell_condition_2 = diff_pct > 30 
            
            if sell_condition_1 or sell_condition_2:
                current_decision_log.append(f"æ¡ä»¶: å–å‡ºæ¡ä»¶æ»¡è¶³ (åˆ†ä½ç‚¹ > 75% [{curr_percentile*100:.1f}%] æˆ–åç¦»åº¦ > 30% [{diff_pct:.1f}%])ã€‚")
                if current_holdings > 0: 
                    signal = "ğŸ”´ å»ºè®®å–å‡º"
                    current_decision_log.append("ç»“æœ: å»ºè®®å–å‡ºã€‚")
                else: 
                    signal = "ğŸ”´ å»ºè®®å–å‡º (æ— æŒä»“)ã€‚"
            
            elif buy_condition_1 or buy_condition_2:
                current_decision_log.append(f"æ¡ä»¶: ä¹°å…¥æ¡ä»¶æ»¡è¶³ (åˆ†ä½ç‚¹ < 20% [{curr_percentile*100:.1f}%] æˆ– PE < 3/5å¹´å‡å€¼)ã€‚")
                
                suppress_by_step = False
                if current_holdings > 0 and last_buy_pe is not None:
                    required_entry_pe = last_buy_pe * (1 - STEP_PERCENT)
                    current_decision_log.append(f"é˜¶æ¢¯ä¹°å…¥æ£€æŸ¥: ä¸Šæ¬¡ä¹°å…¥PE {last_buy_pe:.2f}, ä¸‹æ¬¡ä¹°å…¥PEé˜ˆå€¼ {required_entry_pe:.2f} (è¦æ±‚è·Œå¹… â‰¥ {STEP_PERCENT*100:.0f}%)ã€‚")
                    if curr_pe > required_entry_pe:
                        suppress_by_step = True
                        current_decision_log.append("ç»“æœ: è·Œå¹…ä¸è¶³ 6%ï¼ŒæŠ‘åˆ¶ä¹°å…¥ã€‚")
                else:
                    current_decision_log.append(f"é˜¶æ¢¯ä¹°å…¥æ£€æŸ¥: æ— æŒä»“æˆ–æ— ä¸Šæ¬¡ä¹°å…¥PEï¼Œä¸æ£€æŸ¥è·Œå¹…é™åˆ¶ã€‚")
                        
                if suppress_by_step:
                    signal = "â¸ï¸ è§‚æœ› (è·Œå¹…ä¸è¶³ 6%)"
                elif current_holdings < MAX_UNITS:
                    signal = "ğŸŸ¢ å»ºè®®ä¹°å…¥"
                    current_decision_log.append("ç»“æœ: å»ºè®®ä¹°å…¥ã€‚")
                else:
                    signal = "ğŸŸ¢ å»ºè®®ä¹°å…¥ (å·²æ»¡ä»“)"
                    current_decision_log.append("ç»“æœ: å»ºè®®ä¹°å…¥ (å·²æ»¡ä»“)ã€‚")
            else:
                current_decision_log.append("æ¡ä»¶: æ— æ˜ç¡®ä¹°å…¥/å–å‡ºä¿¡å·ã€‚")
            
        decision_logs[code] = current_decision_log 

        table_data.append({
            "æŒ‡æ•°åç§°": name,
            "å½“å‰PE": f"{curr_pe:.2f}", 
            "PEåˆ†ä½ç‚¹": f"{curr_percentile * 100:.1f}%", 
            "åç¦»åº¦(3å¹´%)": f"{diff_pct:.1f}%" if not np.isnan(diff_pct) else 'â€”', 
            "å»ºè®®ä¿¡å·": signal,
            "ä¸Šæ¬¡æ“ä½œè·ä»Š(å¤©)": days_since_last_op_display,
            "å¹³å‡æˆæœ¬(ETF)": avg_cost_display,
            "æµ®åŠ¨ç›ˆäº(%)": pl_pct_display,
            "å½“å‰æŒä»“(ä»½)": current_holdings,
            "ä¸Šæ¬¡æ“ä½œæ—¥æœŸ": last_date, 
            "ä¸Šæ¬¡æ“ä½œPE": f"{last_pe:.2f}" if not np.isnan(last_pe) else 'â€”', 
        })
    else:
        decision_logs[code] = ["æ•°æ®å¤„ç†å¤±è´¥æˆ–æ–‡ä»¶ç¼ºå¤±ï¼Œæ— æ³•è¯„ä¼°ç­–ç•¥ã€‚"]
        table_data.append({
            "æŒ‡æ•°åç§°": name, 
            "å»ºè®®ä¿¡å·": "âš ï¸ æ•°æ®å¤„ç†å¤±è´¥/æ–‡ä»¶ç¼ºå¤±",
            "PEåˆ†ä½ç‚¹": "â€”", "åç¦»åº¦(3å¹´%)": "â€”", "å½“å‰PE": "â€”", 
            "ä¸Šæ¬¡æ“ä½œè·ä»Š(å¤©)": 'â€”',
            "å¹³å‡æˆæœ¬(ETF)": 'â€”', "æµ®åŠ¨ç›ˆäº(%)": 'â€”',
            "å½“å‰æŒä»“(ä»½)": s["holdings"], "ä¸Šæ¬¡æ“ä½œæ—¥æœŸ": last_date, "ä¸Šæ¬¡æ“ä½œPE": "â€”"
        })

progress_bar.empty()

# --- æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜çŠ¶æ€å’Œé‡æ–°è¿è¡Œ ---
if updated_records_count > 0:
    save_state(state)
    st.success(f"âœ… å·²è‡ªåŠ¨è¡¥å½• {updated_records_count} æ¡äº¤æ˜“è®°å½•çš„ PE/ç‚¹ä½æ•°æ®!")
    st.cache_data.clear() 
    time.sleep(1)
    st.rerun()

# --- æ ¸å¿ƒæŒ‡æ ‡æ˜¾ç¤ºåŒº (é¡¶éƒ¨å¢åŠ æ•´ä½“ä»“ä½æŒ‡æ ‡) ---
st.subheader("æ ¸å¿ƒæŒ‡æ ‡")
col_overall_pos, col_curr_pe, col_percentile, col_deviation = st.columns([1,1,1,1]) 

with col_overall_pos.container(border=True):
    st.markdown("### ğŸŒ æ•´ä½“ä»“ä½æŒ‡æ ‡")
    if not np.isnan(overall_pe_percentile):
        overall_position_str = f"{overall_position_pct:.1f}%"
        st.metric(label="å»ºè®®æ•´ä½“ä»“ä½ (1-å¤§ç›˜åˆ†ä½)", value=overall_position_str)
        if overall_position_pct > 75:
            st.success("å¸‚åœºæ•´ä½“ä½ä¼°ï¼Œå¯ç§¯æå¸ƒå±€ã€‚")
        elif overall_position_pct < 25:
            st.error("å¸‚åœºæ•´ä½“é«˜ä¼°ï¼Œæ³¨æ„é£é™©ã€‚")
        else:
            st.info("å¸‚åœºä¼°å€¼é€‚ä¸­ã€‚")
    else:
        st.info("å¤§ç›˜æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•è®¡ç®—ã€‚")

with col_curr_pe.container(border=True):
    st.markdown(f"### ğŸ¯ å½“å‰æŒ‡æ•°æ€»æ•°")
    st.metric(label="ç›‘æ§ä¸­æŒ‡æ•°æ•°é‡", value=len(TARGETS))
    st.markdown("æ•°æ®é©±åŠ¨æ‚¨çš„åˆ†æ•£æŠ•èµ„å†³ç­–ã€‚")

with col_percentile.container(border=True):
    st.markdown("### ğŸ“ˆ PEåˆ†ä½ç‚¹")
    st.info("ç»¿è‰²: <20% (ä½ä¼°) | çº¢è‰²: >80% (é«˜ä¼°)")
    st.markdown("è¯„ä¼°å½“å‰ä¼°å€¼åœ¨å†å²ä¸Šçš„ç›¸å¯¹ä½ç½®ã€‚")

with col_deviation.container(border=True):
    st.markdown("### ğŸ’¡ å»ºè®®ä¿¡å·")
    st.success("ğŸŸ¢: å»ºè®®ä¹°å…¥")
    st.error("ğŸ”´: å»ºè®®å–å‡º")
    st.markdown("åŸºäºå¤šé‡æŒ‡æ ‡(3/5å¹´å‡å€¼, åˆ†ä½ç‚¹)ç”Ÿæˆçš„ç­–ç•¥å»ºè®®ã€‚")

st.markdown("---")

st.subheader("ğŸ“‹ æŒ‡æ•°ä¼°å€¼ä¸ç­–ç•¥æ€»è§ˆ")

df_display = pd.DataFrame(table_data)

st.dataframe(
    df_display.style
        .applymap(highlight_signal, subset=['å»ºè®®ä¿¡å·'])
        .applymap(highlight_percentile, subset=['PEåˆ†ä½ç‚¹'])
        .applymap(highlight_pl, subset=['æµ®åŠ¨ç›ˆäº(%)']),
    use_container_width=True,
    height=500
)

# --- è¯¦ç»†çš„å†³ç­–ä¾æ®æ˜¾ç¤º (æ‰©å±•åˆ°æ‰€æœ‰æŒ‡æ•°) ---
st.markdown("---")
st.subheader("ğŸ” è¯¦ç»†å†³ç­–ä¾æ® (æ‰€æœ‰æŒ‡æ•°)")
for code, name in TARGETS.items(): 
    with st.expander(f"**{name}** å»ºè®®ä¿¡å·å†³ç­–æ—¥å¿—"):
        if code in decision_logs and decision_logs[code]:
            for log_entry in decision_logs[code]:
                st.markdown(f"- {log_entry}")
        else:
            st.info("æ— å†³ç­–æ—¥å¿—ä¿¡æ¯ã€‚")

# --- äº¤æ˜“ç™»è®°é€»è¾‘ (æ–°å¢äº¤æ˜“ç®¡ç†) ---
st.markdown("---")
st.header("ğŸ›’ äº¤æ˜“ç™»è®°ä¸ç®¡ç†")

tab_record, tab_manage = st.tabs(["ğŸ“ ç™»è®°æ–°äº¤æ˜“", "ğŸ—‘ï¸ ç®¡ç†/ä¿®æ”¹äº¤æ˜“è®°å½•"])

# ======================= Tab 1: ç™»è®°æ–°äº¤æ˜“ =======================
with tab_record:
    st.markdown("ç™»è®°æ–°çš„ä¹°å…¥æˆ–å–å‡ºè®°å½•ã€‚")
    with st.container(border=True):
        col1, col2, col3, col4, col5 = st.columns([2, 1, 2, 2, 2])

        name_options = list(TARGETS.values())

        with col1:
            selected_name_r = st.selectbox("é€‰æ‹©æŒ‡æ•°", name_options, key="select_record_index")
            selected_file_r = [f for f, n in TARGETS.items() if n == selected_name_r][0]

        with col2:
            action_r = st.selectbox("æ“ä½œç±»å‹", ["ä¹°å…¥", "å–å‡º"], key="select_action")
            
        with col3:
            trade_date_r = st.date_input("æˆäº¤æ—¥æœŸ", value=datetime.now().date(), max_value=datetime.now().date(), key="input_date")
            trade_date_str_r = trade_date_r.strftime("%Y-%m-%d")

        with col4:
            price_label_r = "ETF å®é™…æˆäº¤ä»·æ ¼"
            
            # å°è¯•è·å–æœ€è¿‘çš„äº¤æ˜“ä»·æ ¼ä½œä¸ºé»˜è®¤å€¼
            last_buy_price_r = next((t['price'] for t in reversed(state[selected_file_r]['history']) if t.get('type') == 'ä¹°å…¥' and t.get('price') is not None), 1.0000)
            
            trade_price_r = st.number_input(price_label_r, min_value=0.0001, format="%.4f", value=last_buy_price_r, step=0.0001, key="input_price")

        with col5:
            st.markdown("##### ") 
            st.markdown("##### ") 
            if st.button("æäº¤æ–°è®°å½•", type="primary", use_container_width=True):
                s = state[selected_file_r] 
                df_selected_r = full_data_frames.get(selected_file_r)
                
                if df_selected_r is None:
                    st.error(f"âš ï¸ æ— æ³•æäº¤è®°å½•ï¼šæ•°æ®æ–‡ä»¶ {selected_file_r} è¯»å–å¤±è´¥ã€‚")
                    time.sleep(1); st.cache_data.clear(); st.rerun()

                trade_pe_r, trade_close_r = find_pe_by_date(df_selected_r, trade_date_str_r)
                
                saved_pe_r = round(trade_pe_r, 2) if not np.isnan(trade_pe_r) else None
                saved_close_r = round(trade_close_r, 2) if not np.isnan(trade_close_r) else None
                
                is_data_missing_r = saved_pe_r is None or saved_close_r is None
                
                pe_display_str_r = f"{saved_pe_r:.2f}" if not is_data_missing_r else 'N/A'
                    
                transaction_r = {
                    "date": trade_date_str_r,
                    "type": action_r,
                    "pe": saved_pe_r, 
                    "close": saved_close_r,
                    "price": trade_price_r, 
                    "unit": 1 # é»˜è®¤æ¯æ¬¡äº¤æ˜“ä¸€ä»½
                }
                
                if action_r == "ä¹°å…¥":
                    if s["holdings"] < MAX_UNITS:
                        s["history"].append(transaction_r) 
                        save_state(state)
                        st.success(f"å·²è®°å½•ï¼š{selected_name_r} ä¹°å…¥1ä»½ã€‚PE: {pe_display_str_r}, ETFæˆäº¤ä»·: {trade_price_r:.4f}ã€‚å½“å‰æŒä»“ {state[selected_file_r]['holdings']} ä»½ã€‚")
                    else:
                        st.info(f"å·²è¾¾åˆ°æœ€å¤§æŒä»“ä»½æ•° ({MAX_UNITS})ï¼Œæ— æ³•ç»§ç»­ä¹°å…¥ã€‚")
                    
                elif action_r == "å–å‡º":
                    if s["holdings"] > 0:
                        s["history"].append(transaction_r) 
                        save_state(state)
                        st.warning(f"å·²è®°å½•ï¼š{selected_name_r} å–å‡º1ä»½ã€‚PE: {pe_display_str_r}, ETFæˆäº¤ä»·: {trade_price_r:.4f}ã€‚å½“å‰æŒä»“ {state[selected_file_r]['holdings']} ä»½ã€‚")
                    else:
                        st.error("æŒä»“ä¸º0ï¼Œæ— æ³•å–å‡ºã€‚")
                        
                time.sleep(1)
                st.cache_data.clear() 
                st.rerun()

# ======================= Tab 2: ç®¡ç†/ä¿®æ”¹äº¤æ˜“è®°å½• =======================
with tab_manage:
    st.markdown("âš ï¸ **å±é™©æ“ä½œï¼** åˆ é™¤å’Œä¿®æ”¹è®°å½•å°†ç›´æ¥å½±å“æŒä»“å’Œæˆæœ¬è®¡ç®—ã€‚")

    name_options_m = list(TARGETS.values())
    selected_name_m = st.selectbox("é€‰æ‹©è¦ç®¡ç†è®°å½•çš„æŒ‡æ•°", name_options_m, key="select_manage_index")
    selected_file_m = [f for f, n in TARGETS.items() if n == selected_name_m][0]
    
    s_m = state[selected_file_m]
    
    if not s_m['history']:
        st.info("è¯¥æŒ‡æ•°å°šæ— äº¤æ˜“è®°å½•å¯ä¾›ç®¡ç†ã€‚")
    else:
        # å°†å†å²è®°å½•è½¬æ¢ä¸º DataFrame ä»¥ä¾¿å±•ç¤ºç´¢å¼•
        history_df_m = pd.DataFrame(s_m['history'])
        history_df_m['index'] = history_df_m.index # æ·»åŠ ç´¢å¼•åˆ—ç”¨äºè¯†åˆ«è®°å½•
        
        # ç®€åŒ– DataFrame ä»¥ä¾› Streamlit å±•ç¤º
        df_display_m = history_df_m[['index', 'date', 'type', 'price', 'unit', 'pe', 'close']].copy()
        df_display_m = df_display_m.rename(columns={'index': 'ç´¢å¼•', 'date': 'æˆäº¤æ—¥æœŸ', 'type': 'æ“ä½œç±»å‹', 'price': 'ETFæˆäº¤ä»·', 'unit': 'äº¤æ˜“ä»½æ•°', 'pe': 'æˆäº¤PE(è‡ªåŠ¨)', 'close': 'æˆäº¤ç‚¹ä½(è‡ªåŠ¨)'})
        
        # æ ¼å¼åŒ– PE/Close/Priceï¼Œé˜²æ­¢ NaN æ˜¾ç¤ºé—®é¢˜
        for col in ['ETFæˆäº¤ä»·', 'æˆäº¤PE(è‡ªåŠ¨)', 'æˆäº¤ç‚¹ä½(è‡ªåŠ¨)']:
             if col in df_display_m.columns:
                if col == 'ETFæˆäº¤ä»·':
                    df_display_m[col] = df_display_m[col].apply(lambda x: f"{x:.4f}" if pd.notna(x) and x is not None else 'N/A')
                else:
                    df_display_m[col] = df_display_m[col].apply(lambda x: f"{x:.2f}" if pd.notna(x) and x is not None else 'N/A')
        
        st.dataframe(df_display_m, use_container_width=True)

        # --- åˆ é™¤æ“ä½œ ---
        st.subheader("åˆ é™¤è®°å½•")
        col_del, col_button_del = st.columns([1, 1])
        with col_del:
            index_to_delete = st.number_input("è¾“å…¥è¦åˆ é™¤çš„è®°å½•è¡Œç´¢å¼• (æœ€å·¦ä¾§åˆ—)", min_value=0, max_value=len(history_df_m) - 1, step=1, key="delete_index")
        
        with col_button_del:
            st.markdown("##### ")
            if st.button(f"ğŸ”´ ç¡®è®¤åˆ é™¤ç¬¬ {index_to_delete} è¡Œè®°å½•", key="confirm_delete_button"):
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                if 0 <= index_to_delete < len(s_m['history']):
                    del s_m['history'][index_to_delete]
                    save_state(state)
                    st.success(f"âœ… è®°å½• {index_to_delete} å·²åˆ é™¤ï¼ŒæŒä»“å·²é‡æ–°è®¡ç®—ã€‚")
                    time.sleep(1)
                    st.cache_data.clear() 
                    st.rerun()
                else:
                    st.error("ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè¯·æ£€æŸ¥è¾“å…¥ã€‚")

        st.markdown("---")
        
        # --- ä¿®æ”¹æ“ä½œ (ä»…æ”¯æŒä¿®æ”¹ä»·æ ¼/æ—¥æœŸ/ç±»å‹/ä»½æ•°) ---
        st.subheader("ä¿®æ”¹è®°å½•")
        
        df_selected_m = full_data_frames.get(selected_file_m)
        if df_selected_m is None:
            st.error("æ— æ³•è·å–æŒ‡æ•°æ•°æ®ï¼Œæ— æ³•è¿›è¡Œä¿®æ”¹ã€‚è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨ã€‚")
        else:
            col_mod_index, col_mod_type, col_mod_date, col_mod_price = st.columns(4)
            
            # 1. é€‰æ‹©è¦ä¿®æ”¹çš„ç´¢å¼•
            with col_mod_index:
                index_to_modify = st.number_input("è¾“å…¥è¦ä¿®æ”¹çš„è®°å½•è¡Œç´¢å¼•", min_value=0, max_value=len(history_df_m) - 1, step=1, key="modify_index")
            
            # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
            if 0 <= index_to_modify < len(s_m['history']):
                record_to_modify = s_m['history'][index_to_modify]
                
                # 2. ä¿®æ”¹æ“ä½œç±»å‹
                with col_mod_type:
                    new_type = st.selectbox("æ–°æ“ä½œç±»å‹", ["ä¹°å…¥", "å–å‡º"], index=0 if record_to_modify.get('type') == 'ä¹°å…¥' else 1, key="modify_type")

                # 3. ä¿®æ”¹æˆäº¤æ—¥æœŸ
                with col_mod_date:
                    try:
                        current_date = datetime.strptime(record_to_modify.get('date'), "%Y-%m-%d").date()
                    except:
                         current_date = datetime.now().date()
                    new_date = st.date_input("æ–°æˆäº¤æ—¥æœŸ", value=current_date, max_value=datetime.now().date(), key="modify_date")
                    new_date_str = new_date.strftime("%Y-%m-%d")

                # 4. ä¿®æ”¹ ETF ä»·æ ¼
                with col_mod_price:
                    new_price = st.number_input("æ–°ETFæˆäº¤ä»·", min_value=0.0001, format="%.4f", value=record_to_modify.get('price', 1.0000), step=0.0001, key="modify_price")
                    
                # 5. ä¿®æ”¹ä»½æ•° (é»˜è®¤ä¸º 1)
                new_unit = st.number_input("æ–°äº¤æ˜“ä»½æ•°", min_value=1, value=record_to_modify.get('unit', 1), step=1, key="modify_unit")

                if st.button(f"ğŸŸ¡ ç¡®è®¤ä¿®æ”¹ç¬¬ {index_to_modify} è¡Œè®°å½•", key="confirm_modify_button"):
                    
                    # é‡æ–°æŸ¥æ‰¾æ–°çš„ PE/Close
                    trade_pe_mod, trade_close_mod = find_pe_by_date(df_selected_m, new_date_str)
                    saved_pe_mod = round(trade_pe_mod, 2) if not np.isnan(trade_pe_mod) else None
                    saved_close_mod = round(trade_close_mod, 2) if not np.isnan(trade_close_mod) else None

                    # æ›´æ–°è®°å½•
                    s_m['history'][index_to_modify] = {
                        "date": new_date_str,
                        "type": new_type,
                        "pe": saved_pe_mod, 
                        "close": saved_close_mod,
                        "price": new_price, 
                        "unit": new_unit
                    }

                    save_state(state)
                    st.success(f"âœ… è®°å½• {index_to_modify} å·²æ›´æ–°å¹¶é‡æ–°è®¡ç®—æŒä»“ã€‚")
                    time.sleep(1)
                    st.cache_data.clear() 
                    st.rerun()
            else:
                st.error("ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè¯·æ£€æŸ¥è¾“å…¥ã€‚")
